{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based method for Machine Learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Species of tree-based models\n",
    "\n",
    "- ID3\n",
    "- c4.5\n",
    "- C5.0\n",
    "- CART (most used)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CART pseudocode\n",
    "\n",
    "l: current minimum loss at a given node\n",
    "\n",
    "j*: index of column in X that is current best predictor\n",
    "\n",
    "- trying to classify email as spam or not-spam, and we look at number of features (punctuation errors, number of dollar signs, etc.)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Advantages of Tree-based methods\n",
    "\n",
    "- intuitive, like flowchart\n",
    "- viable when n << p (observations << features)\n",
    "    - not usually the case for regression models\n",
    "- handle interactions naturally\n",
    "- minimal assumptions\n",
    "- handle missingness in predictors\n",
    "    - usually finding surrogate variables that behaves similarly to the variable with missing data\n",
    "\n",
    "Disadvantages\n",
    "- high variance (i.e. we can't necessarily apply the same model elsewhere)\n",
    "    - hyper-specific to fitting the training data (\"prone to overfitting\")\n",
    "- lack of smoothness (troubling for regression)\n",
    "    - we end up having a stairstep look"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bootstrap Aggregation\n",
    "\n",
    "- Extends idea of CART models\n",
    "\n",
    "- since single trees overfit, we can build lots of trees and take the mean of their predicted value\n",
    "    - we let each tree essentially \"cast a vote\" for the outcome variable\n",
    "    \n",
    " - sampling from the sample i.e. treating the sample as the population\n",
    " \n",
    " - this bootstrapping allows us to fit to different subsets of the same data, over and over again. \n",
    " \n",
    " Advantages of bagging\n",
    " \n",
    " - big improvement in predictive performance\n",
    " - variance decreases\n",
    " - easy to tune (i.e. the only thing we're setting is the number of trees we are setting i.e. how many bootstrapping iterations)\n",
    " - embarrassingly parallel\n",
    "     - this feature can make the process execute much more efficiently and quickly\n",
    "- out-of-bag (OOB) error estimate (more on this later)\n",
    "    - essentially a proxy for the performance of the model\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Random Forests\n",
    "\n",
    "- Extends the idea of bagging\n",
    "    - build many trees, aggregate predictions\n",
    "- only consider m predictors at each node, which then improves predictive performance\n",
    "- improve predictive performance\n",
    "    - reduce inter-tree correlation (i.e. injecting more randomness in the model)\n",
    "    - further reduce variance beyond bagging\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "OOB Samples\n",
    "\n",
    "- a feature of both bagging and random forests\n",
    "- some observations get left out of each sample, and is nice because it induces randomness\n",
    "    - those \"out of bag\" samples let us test the quality of the model, i.e. the error\n",
    "           - they are a sort of quasi-test set\n",
    "- OOB error and test error behave very similarly\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Variable importance\n",
    "\n",
    "- in machine learning, unlike classical stats, we are less concerned with specifics of variables, but more concerned with the relative importance of one variable vs another (i.e. not so much statistical significance or p-value)\n",
    "\n",
    "- OOB samples allow us to estimate this relative importance\n",
    "    - Gini improvement\n",
    "    - OOB randomization\n",
    "        - how much better than random noise is out prediction doing?\n",
    "        - shuffle the X's and compute the decrease in prediction accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Overfitting\n",
    "\n",
    "- it was once mistakenly thought that random forests would not overfit\n",
    "\n",
    "- you CAN overfit by growing deep trees\n",
    "\n",
    "- growing too many trees won't cause you to oerfit (but it's wasteful of computing resources)\n",
    "\n",
    "\n",
    "- in general, people worry MUCHless about overfitting in random forests as opposed to, say, a single CART tree\n",
    "\n",
    "\n",
    "Tree Depth\n",
    "\n",
    "- Tree depth can be controlled in two ways:\n",
    "    - set max number of terminal noed (or interaction depth)\n",
    "        - in scikit-learn, max_depth\n",
    "- specifying minimum nuimber of observations in a node \n",
    "    - in scikit learn, min_samples_split\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Advantage of Random forests\n",
    "\n",
    "- powerful prediction models\n",
    "- easy to tune\n",
    "- difficult to overfit (more trees won't overfit)\n",
    "- variable importance\n",
    "- embarassingly parallel (unlike boosting)\n",
    "- out-of-bag (OOB) error estimate\n",
    "    - excellent approximate of test error\n",
    "    - no need for cross validation\n",
    "    \n",
    "    \n",
    "Disadvantages of random forests\n",
    "\n",
    "- viewed as black box approach\n",
    "- variable importance must be interpreted cautiously\n",
    "    - less interpretable than parameter estimates in parametric models\n",
    "    - cannot meaningfully compare random forest to boosting\n",
    "    \n",
    "- Trouble with severe outliers\n",
    "    - in regression, training data constrains the max or min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
